{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHvCz3sljEAQAvaQKw/UDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/potis/AISummit/blob/main/AI_Summit_2023_Part_1_MNIST_Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope\n",
        "\n",
        "Designing an image classification system that generates uncertainty estimates and performs out-of- distribution detection"
      ],
      "metadata": {
        "id": "JL1f1sbm4MLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries\n"
      ],
      "metadata": {
        "id": "obJxClyn4QEQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKxMUrCO4JaZ",
        "outputId": "eed4011d-95e3-4e58-ca65-64e1398ec4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.65.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (8.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.15.2+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (23.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->medmnist) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->medmnist) (16.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->medmnist) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->medmnist) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autokeras in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autokeras) (23.1)\n",
            "Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from autokeras) (2.12.0)\n",
            "Requirement already satisfied: keras-tuner>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from autokeras) (1.3.5)\n",
            "Requirement already satisfied: keras-nlp>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from autokeras) (0.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from autokeras) (1.5.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp>=0.4.0->autokeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp>=0.4.0->autokeras) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras-nlp>=0.4.0->autokeras) (2.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner>=1.1.0->autokeras) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner>=1.1.0->autokeras) (1.0.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras) (0.32.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->autokeras) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->autokeras) (2022.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->autokeras) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.8.0->autokeras) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.8.0->autokeras) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (2.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (3.4)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp>=0.4.0->autokeras) (0.13.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.8.0->autokeras) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pip --upgrade\n",
        "!pip install medmnist\n",
        "!pip install autokeras\n",
        "!pip install numpy\n",
        "!pip install scikeras\n",
        "!pip install -U -q --use-deprecated=legacy-resolver tf-models-official tensorflow\n",
        "!pip install tensorflow_probability\n",
        "!pip install --upgrade tf_agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "6AUjrjBa5eVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import time\n",
        "import medmnist\n",
        "import random\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import *\n",
        "from sklearn import metrics as sklearn_metrics\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    result = y_pred\n",
        "    cf_matrix = sklearn_metrics.confusion_matrix(y_true, result)\n",
        "\n",
        "    cfm_labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "    cfm_values = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "    cfm_percent = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cfm_labels, cfm_values, cfm_percent)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
        "    ax.set_title('Classifier\\n\\n')\n",
        "    ax.set_xlabel('\\nPredicted Values')\n",
        "    ax.set_ylabel('Actual Values')\n",
        "\n",
        "    # Ticket labels - List must be in alphabetical order\n",
        "    ax.xaxis.set_ticklabels(['False', 'True'])\n",
        "    ax.yaxis.set_ticklabels(['False', 'True'])\n",
        "\n",
        "    # Display the visualization of the Confusion Matrix.\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) =keras.datasets.mnist.load_data()\n",
        "print(np.shape(train_data))\n",
        "print(np.shape(train_labels))\n",
        "\n",
        "train_filter = np.where((train_labels == 0 ) | (train_labels == 4))\n",
        "test_filter = np.where((test_labels == 0) | (test_labels == 4))\n",
        "ood_filter= np.where((test_labels == 9) )\n",
        "\n",
        "\n",
        "X_train, Y_train = train_data[train_filter], train_labels[train_filter]\n",
        "Y_train[Y_train==4]=1\n",
        "Y_train_c=to_categorical(Y_train)\n",
        "X_test, Y_test = test_data[test_filter], test_labels[test_filter]\n",
        "Y_test[Y_test==4]=1\n",
        "\n",
        "ood_test= test_data[ood_filter]\n",
        "\n",
        "print(np.shape(Y_train))\n",
        "print(np.shape(X_train))\n",
        "print(Y_train.max())"
      ],
      "metadata": {
        "id": "5mgEDLGZ4WZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show some samples image"
      ],
      "metadata": {
        "id": "KBLGKZiv54ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.title(str(Y_train[2]))\n",
        "plt.imshow(X_train[2, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.title(str(Y_train[-1]))\n",
        "\n",
        "plt.imshow(X_train[-1, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.imshow(ood_test[-1, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hrIS_2MK536Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model"
      ],
      "metadata": {
        "id": "vfkVVkf95igu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAq9xcS48mRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 28\n",
        "img_width = 28\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "class MyModel(models.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Rescaling(1./255, input_shape=(28, 28, 1)))\n",
        "        model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "        model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "        model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(16, activation='relu'))\n",
        "        model.add(layers.Dropout(.25))\n",
        "        model.add(layers.Dense(2, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "model = MyModel()\n",
        "model.build(((None,None,None, 1)))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "              learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "epochs=5\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "oPMAq-yh4y1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model"
      ],
      "metadata": {
        "id": "lVsSncOKKU0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=model(X_test)\n",
        "predictions=predictions.numpy()\n",
        "print(np.shape(predictions))\n",
        "\n",
        "predictions_bin=predictions.argmax(axis=-1)\n",
        "print(np.shape(predictions))\n",
        "plot_confusion_matrix(Y_test, predictions_bin)\n",
        "\n",
        "plt.title('True class vs Prediction')\n",
        "plt.scatter(Y_test, predictions[:,1], c= list(Y_test), alpha=0.9)\n",
        "plt.show()\n",
        "\n",
        "plt.title('True class vs Uncertainty')\n",
        "\n",
        "uncertainty =  predictions[:,1] * (1 -  predictions[:,1])\n",
        "plt.scatter(Y_test, uncertainty, c= list(Y_test), alpha=0.9)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jFHZtkBTKUMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model in a sample originating from the test set"
      ],
      "metadata": {
        "id": "6hfjbyGQEB4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_uncertainty = resnet_probs * (1 - resnet_probs)\n",
        "# Get a sample\n",
        "example_case=10\n",
        "sample=X_test[example_case, :,:]\n",
        "plt.figure()\n",
        "plt.imshow(sample, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "predictions_sample=model(sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "print(np.shape(predictions_sample))\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "print('Indistribution sample')\n",
        "print(predictions_sample)\n",
        "uncertainty = predictions_sample * (1 - predictions_sample)\n",
        "print(uncertainty)\n",
        "# What if we just rotate?\n",
        "print('Rotating image 90 degrees')\n",
        "rot_sample=np.rot90(X_test[example_case, :,:],1).reshape((1,28,28))\n",
        "predictions_sample=model(rot_sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "\n",
        "print(predictions_sample)\n",
        "uncertainty = predictions_sample * (1 - predictions_sample)\n",
        "print(uncertainty)\n",
        "rot_sample=np.flipud(X_test[example_case, :,:]).reshape((1,28,28))\n",
        "predictions_sample=model(rot_sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "\n",
        "print('Fliping image upside down')\n",
        "print(predictions_sample)\n",
        "uncertainty = predictions_sample * (1 - predictions_sample)\n",
        "print(uncertainty)"
      ],
      "metadata": {
        "id": "viMnCH6eENqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model in out of distribution data (Deterministic Model)"
      ],
      "metadata": {
        "id": "yozys_UcD7Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_uncertainty = resnet_probs * (1 - resnet_probs)\n",
        "# Get a sample\n",
        "sample=ood_test[4, :,:]\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "predictions=model(sample)\n",
        "predictions=predictions.numpy()\n",
        "predictions=predictions[0,1]\n",
        "print(predictions)\n",
        "uncertainty = predictions * (1 - predictions)\n",
        "print(uncertainty)"
      ],
      "metadata": {
        "id": "sKuiTDzWD_1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply to all the OOD and plot uncertainty"
      ],
      "metadata": {
        "id": "SnFYpiA6I3xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=model(ood_test)\n",
        "predictions=predictions.numpy()\n",
        "predictions=predictions[:,1]\n",
        "\n",
        "uncertainty = predictions * (1 - predictions)\n",
        "plt.title('OOD Uncertainty')\n",
        "uncertainty = predictions * (1 - predictions)\n",
        "plt.plot(uncertainty)\n",
        "plt.axhline(y=uncertainty.mean(),color = 'r', linestyle = '-')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IYT-i2J0Iv1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply to all the test cases and plot uncertainty"
      ],
      "metadata": {
        "id": "xoXDj4QuJcKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=model(X_test)\n",
        "predictions=predictions.numpy()\n",
        "predictions=predictions[:,1]\n",
        "\n",
        "uncertainty = predictions * (1 - predictions)\n",
        "plt.title('Test Set Uncertainty')\n",
        "uncertainty = predictions * (1 - predictions)\n",
        "plt.plot(uncertainty)\n",
        "plt.axhline(y=uncertainty.mean(),color = 'r', linestyle = '-')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "atqEzcOwJZhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo dropout"
      ],
      "metadata": {
        "id": "wyYBlW5X-2kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ensemble = 100\n",
        "def mc_dropout_sampling(test_examples):\n",
        "  # Enable dropout during inference.\n",
        "  return model(test_examples, training=True)"
      ],
      "metadata": {
        "id": "D1VpiVoW70dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In distribution"
      ],
      "metadata": {
        "id": "9zO6oNdWAUi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample\n",
        "sample=X_test[0,:,:]\n",
        "print(f\"True Class {Y_test[0]}\")\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "dropout_samples = [mc_dropout_sampling(sample).numpy()[0,1] for _ in range(num_ensemble)]\n",
        "print(np.array(dropout_samples).mean())\n",
        "uncertainty = np.array(dropout_samples).mean() * (1 - np.array(dropout_samples).mean())\n",
        "print(uncertainty)\n",
        "# Get a sample\n",
        "sample=X_test[10,:,:]\n",
        "print(f\"True Class {Y_test[10]}\")\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "dropout_samples = [mc_dropout_sampling(sample).numpy()[0,1] for _ in range(num_ensemble)]\n",
        "print(np.array(dropout_samples).mean())\n",
        "uncertainty = np.array(dropout_samples).mean() * (1 - np.array(dropout_samples).mean())\n",
        "print(uncertainty)\n"
      ],
      "metadata": {
        "id": "X_SJuDzxAmQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Out of distribution example"
      ],
      "metadata": {
        "id": "6lfIamHSAbNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(ood_test[10, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "sample_ood=ood_test[4,:,:]\n",
        "sample_ood=np.reshape(sample_ood,(1,28,28))\n",
        "dropout_samples_ood = [mc_dropout_sampling(sample_ood).numpy()[0,1] for _ in range(num_ensemble)]\n",
        "print(dropout_samples_ood)\n",
        "print(np.array(dropout_samples_ood).mean())\n",
        "uncertainty = np.array(dropout_samples_ood).mean() * (1 - np.array(dropout_samples_ood).mean())\n",
        "print(uncertainty)\n"
      ],
      "metadata": {
        "id": "Gj0PhqQuAgBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Ensemble"
      ],
      "metadata": {
        "id": "1-www8SdCaeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ensemble=3\n",
        "# Deep ensemble training\n",
        "deep_ensemble = []\n",
        "import random\n",
        "\n",
        "\n",
        "for ivar in range(num_ensemble):\n",
        "  print(f'{ivar}')\n",
        "  tf.keras.utils.set_random_seed(random.randint(100,1000))\n",
        "  temp_model= MyModel()\n",
        "  temp_model.build(((None,None,None, 1)))\n",
        "\n",
        "  temp_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "  temp_model.reset_states()\n",
        "  temp_model.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=epochs)\n",
        "  print(f'------')\n",
        "\n",
        "  deep_ensemble.append(temp_model)\n",
        "\n",
        "  del temp_model"
      ],
      "metadata": {
        "id": "xNcw40pcBg7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get a sample\n",
        "sample=X_test[-1,:,:]\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "ensemble_preds_samples = [model_x(sample).numpy()[0,1] for model_x in (deep_ensemble)]\n",
        "print(np.array(ensemble_preds_samples).mean())\n",
        "uncertainty = np.array(ensemble_preds_samples).mean() * (1 - np.array(ensemble_preds_samples).mean())\n",
        "print(uncertainty)\n",
        "\n",
        "\n",
        "for i in range(0,10):\n",
        "  sample_ood=ood_test[i,:,:]\n",
        "  plt.figure()\n",
        "  plt.imshow(sample_ood, cmap='gray')\n",
        "  plt.colorbar()\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "  sample_ood=np.reshape(sample_ood,(1,28,28))\n",
        "  ensemble_preds_samples_ood = [model_x(sample_ood).numpy()[0,1] for model_x in (deep_ensemble)]\n",
        "  print(np.array(ensemble_preds_samples_ood).mean())\n",
        "  uncertainty = np.array(ensemble_preds_samples_ood).mean() * (1 - np.array(ensemble_preds_samples_ood).mean())\n",
        "  print(uncertainty)"
      ],
      "metadata": {
        "id": "7rErGkICDAA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import official.nlp.modeling.layers as nlp_layers\n",
        "epochs=10\n",
        "class simplenet(tf.keras.Model):\n",
        "    \"\"\"Defines a multi-layer residual network.\"\"\"\n",
        "    def __init__(self, num_classes=1, num_layers=1, num_hidden=32,\n",
        "                 dropout_rate=0.1, **classifier_kwargs):\n",
        "        super().__init__()\n",
        "        # Define class meta data.\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.classifier_kwargs = classifier_kwargs\n",
        "\n",
        "        # Define the layers.\n",
        "        self.input_layer = tf.keras.layers.Input((None,None))\n",
        "        self.normalize_layer = tf.keras.layers.Rescaling(1/255.)\n",
        "        self.reshape_layer = tf.keras.layers.Reshape((28, 28, 1))\n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')\n",
        "        self.maxpool1 = tf.keras.layers.MaxPool2D(2)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "        self.maxpool2 = tf.keras.layers.MaxPool2D(2)\n",
        "        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')\n",
        "        self.maxpool3 = tf.keras.layers.MaxPool2D(2)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(num_hidden)\n",
        "        self.dense_layers = [self.make_dense_layer() for _ in range(num_layers)]\n",
        "        self.classifier = self.make_output_layer(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Project the 2D input data to high dimension.\n",
        "        normalized = self.normalize_layer(inputs)\n",
        "        reshape_layer=self.reshape_layer(normalized)\n",
        "        conv1 = self.conv1(reshape_layer)\n",
        "        maxpool1 = self.maxpool1(conv1)\n",
        "        conv2 = self.conv2(maxpool1)\n",
        "        maxpool2 = self.maxpool2(conv2)\n",
        "        conv3 = self.conv3(maxpool2)\n",
        "        maxpool3 = self.maxpool3(conv3)\n",
        "        flattened = self.flatten(maxpool3)\n",
        "        dense = self.dense(flattened)\n",
        "\n",
        "        # Compute the ResNet hidden representations.\n",
        "        for i in range(self.num_layers):\n",
        "            resid = self.dense_layers[i](dense)\n",
        "            resid = tf.keras.layers.Dropout(self.dropout_rate)(resid)\n",
        "            dense += resid\n",
        "\n",
        "        return self.classifier(dense)\n",
        "\n",
        "    def make_dense_layer(self):\n",
        "        \"\"\"Use the Dense layer as the hidden layer.\"\"\"\n",
        "        return tf.keras.layers.Dense(self.num_hidden, activation=\"relu\")\n",
        "\n",
        "    def make_output_layer(self, num_classes):\n",
        "        \"\"\"Use the Dense layer as the output layer.\"\"\"\n",
        "        return tf.keras.layers.Dense(\n",
        "            num_classes, **self.classifier_kwargs)\n",
        "\n",
        "\n",
        "\n",
        "# resnet_model = simplenet()\n",
        "# resnet_model.build((None, None))\n",
        "# resnet_model.summary()\n",
        "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# metrics = tf.keras.metrics.CategoricalAccuracy(),\n",
        "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
        "\n",
        "# train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "\n",
        "# resnet_model.compile(**train_config)\n",
        "# resnet_model.fit(\n",
        "#     X_train,\n",
        "#     Y_train,\n",
        "#     validation_split=0.3,\n",
        "#     epochs=epochs)\n"
      ],
      "metadata": {
        "id": "hXGdSvnk_w0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_filter = np.where((Y_test == 0))\n",
        "# X_test_0=X_test[test_filter]\n",
        "# output = resnet_model(X_test_0)\n",
        "# plt.plot(output)\n",
        "# uncertainty = output * (1. - output)\n",
        "# plt.plot(uncertainty, c='r')\n",
        "# plt.title('Test Set')"
      ],
      "metadata": {
        "id": "3GIrp_ViUu4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class simplenetSNGP(simplenet):\n",
        "  def __init__(self, spec_norm_bound=0.9, **kwargs):\n",
        "    self.spec_norm_bound = spec_norm_bound\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def make_dense_layer(self):\n",
        "    \"\"\"Applies spectral normalization to the hidden layer.\"\"\"\n",
        "    dense_layer = super().make_dense_layer()\n",
        "    return nlp_layers.SpectralNormalization(\n",
        "        dense_layer, norm_multiplier=self.spec_norm_bound)\n",
        "\n",
        "  def make_output_layer(self, num_classes):\n",
        "    \"\"\"Uses Gaussian process as the output layer.\"\"\"\n",
        "    return nlp_layers.RandomFeatureGaussianProcess(\n",
        "        2,\n",
        "        gp_cov_momentum=-1,\n",
        "        **self.classifier_kwargs)\n",
        "\n",
        "  def call(self, inputs, training=False, return_covmat=False):\n",
        "    # Gets logits and a covariance matrix from the GP layer.\n",
        "    logits, covmat = super().call(inputs)\n",
        "\n",
        "    # Returns only logits during training.\n",
        "    if not training and return_covmat:\n",
        "      return logits, covmat\n",
        "\n",
        "    return logits\n",
        "# sngp_model = simplenetSNGP(**resnet_config)\n",
        "# sngp_model.build((None, None))\n",
        "# sngp_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResetCovarianceCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    \"\"\"Resets covariance matrix at the beginning of the epoch.\"\"\"\n",
        "    if epoch > 0:\n",
        "      self.model.classifier.reset_covariance_matrix()\n",
        "class DeepResNetSNGPWithCovReset(simplenetSNGP):\n",
        "  def fit(self, *args, **kwargs):\n",
        "    \"\"\"Adds ResetCovarianceCallback to model callbacks.\"\"\"\n",
        "    kwargs[\"callbacks\"] = list(kwargs.get(\"callbacks\", []))\n",
        "    kwargs[\"callbacks\"].append(ResetCovarianceCallback())\n",
        "\n",
        "    return super().fit(*args, **kwargs)\n",
        "sngp_model = simplenetSNGP(**resnet_config)\n",
        "sngp_model.build((None,None, None,1))\n",
        "sngp_model.summary()\n",
        "resnet_config = dict(num_classes=2)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
        "train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "sngp_model.compile(**train_config)\n",
        "sngp_model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    validation_split=0.3,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "id": "w2HtZTOlKqm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_posterior_mean_probability(logits, covmat, lambda_param=np.pi / 8.):\n",
        "  # Computes uncertainty-adjusted logits using the built-in method.\n",
        "  logits_adjusted = nlp_layers.gaussian_process.mean_field_logits(\n",
        "      logits, covmat, mean_field_factor=lambda_param)\n",
        "\n",
        "  return tf.nn.softmax(logits_adjusted, axis=-1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EaPQml4OLN0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_filter = np.where((Y_test == 0))\n",
        "X_test_0=X_test[test_filter]\n",
        "sngp_logits, sngp_covmat = sngp_model(X_test_0, return_covmat=True)\n",
        "\n",
        "sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)\n",
        "sngp_probs=sngp_probs[:,1]\n",
        "print(np.shape(sngp_probs))\n",
        "plt.plot(sngp_probs)\n",
        "uncertainty = sngp_probs * (1. - sngp_probs)\n",
        "plt.plot(uncertainty, c='r')\n",
        "plt.title('Test Set')\n",
        "plt.show()\n",
        "sngp_logits_ood, sngp_covmat_ood = sngp_model(ood_test, return_covmat=True)\n",
        "sngp_probs_ood = compute_posterior_mean_probability(sngp_logits_ood, sngp_covmat_ood)[:,1]\n",
        "plt.plot(sngp_probs_ood)\n",
        "uncertainty = sngp_probs_ood * (1. - sngp_probs_ood)\n",
        "plt.plot(uncertainty, c='r')\n",
        "plt.title('OOD Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2QM6Adk9s7X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sngp_logits, sngp_covmat = sngp_model(X_test, return_covmat=True)\n",
        "sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(Y_test,sngp_probs.numpy().argmax(axis=-1) )"
      ],
      "metadata": {
        "id": "8rQ1trmvZAZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Neural Network -Variational inference (VI) as an approximative Bayes approach"
      ],
      "metadata": {
        "id": "-B-md4PW2JdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train a bayesian neural network via variational inference. We again use a CNN with two convolutional blocks, followed by maxpooling layers. The setting is the same as above.\n",
        "\n",
        "\n",
        "\n",
        "The main idea of the Bayes approach in DL is that with BNNs, each weight is replaced\n",
        "by a distribution. Normally, this is quite a complicated distribution, and this distribution\n",
        "isnâ€™t independent among different weights. The idea behind the VI Bayes\n",
        "method is that the complicated posterior distributions of the weights are approximated\n",
        "by a simple distribution called variational distribution."
      ],
      "metadata": {
        "id": "1wwM3YUW2WGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bBQWqbO-2MOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "disable_eager_execution()\n",
        "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X_train.shape[0] *1.0)\n",
        "\n",
        "model_vi = Sequential()\n",
        "model_vi.add(layers.Rescaling(1./255, input_shape=(28, 28, 1)))\n",
        "model_vi.add(tfp.layers.Convolution2DFlipout(16,kernel_size=(3,3),padding=\"same\", activation = 'relu', kernel_divergence_fn=kernel_divergence_fn,input_shape=(28,28,1)))\n",
        "model_vi.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "model_vi.add(tfp.layers.Convolution2DFlipout(32,kernel_size=(3,3),padding=\"same\", activation = 'relu', kernel_divergence_fn=kernel_divergence_fn))\n",
        "model_vi.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "model_vi.add(tfp.layers.Convolution2DFlipout(64,kernel_size=(3,3),padding=\"same\", activation = 'relu', kernel_divergence_fn=kernel_divergence_fn))\n",
        "model_vi.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "model_vi.add(tf.keras.layers.Flatten())\n",
        "model_vi.add(tfp.layers.DenseFlipout(32, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn))\n",
        "model_vi.add(tfp.layers.DenseFlipout(2, activation = 'softmax', kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "X_train=X_train.reshape((-1,28,28,1))\n",
        "\n",
        "model_vi.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['categorical_accuracy'])\n",
        "epochs=10\n",
        "print(np.shape(Y_train))\n",
        "\n",
        "print(np.shape(Y_train))\n",
        "model_vi.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "5HeMls-rS8sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,5):\n",
        "  print(model_vi.predict(X_test[0:1].reshape(1,28,28,1))[0])\n",
        "\n",
        "\n",
        "for i in range(0,5):\n",
        "  print(model_vi.predict(ood_test[0:1].reshape(1,28,28,1))[0])"
      ],
      "metadata": {
        "id": "QaLrg6rq7f8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t0AwbLlo7foY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whats Next?\n",
        "\n",
        "\n",
        "1. Try the same experiment with different digits!\n",
        "2. What about a medical dataset?\n",
        "  - Create a copy of the notebook\n",
        "  - Use the following code to load the data\n",
        "  ```\n",
        "    import os\n",
        "    import matplotlib.pyplot as plt\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    import numpy as np\n",
        "    import time\n",
        "    import medmnist\n",
        "    import random\n",
        "    from sklearn.calibration import CalibratedClassifierCV\n",
        "    from medmnist import INFO, Evaluator\n",
        "    from medmnist.info import DEFAULT_ROOT\n",
        "    from sklearn.metrics import *\n",
        "    from sklearn import metrics as sklearn_metrics\n",
        "    import seaborn as sns\n",
        "    data_flag_class3 = \"pathmnist\"\n",
        "    data_flag_class2 = \"breastmnist\"\n",
        "    data_flag_class1 = \"pneumoniamnist\"\n",
        "\n",
        "    output_root =\"./ood\"\n",
        "\n",
        "    input_root = DEFAULT_ROOT\n",
        "\n",
        "    def plot_confusion_matrix(y_true, y_pred):\n",
        "        result = np.zeros(np.shape(y_pred)[0])\n",
        "        result[y_pred[:,0] < 0.5] = 0\n",
        "        result[y_pred[:,0] >= 0.5] = 1\n",
        "        cf_matrix = sklearn_metrics.confusion_matrix(y_true, result)\n",
        "\n",
        "        cfm_labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "        cfm_values = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "        cfm_percent = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cfm_labels, cfm_values, cfm_percent)]\n",
        "        labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "        ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
        "        ax.set_title('Classifier\\n\\n')\n",
        "        ax.set_xlabel('\\nPredicted Values')\n",
        "        ax.set_ylabel('Actual Values')\n",
        "\n",
        "        # Ticket labels - List must be in alphabetical order\n",
        "        ax.xaxis.set_ticklabels(['False', 'True'])\n",
        "        ax.yaxis.set_ticklabels(['False', 'True'])\n",
        "\n",
        "        # Display the visualization of the Confusion Matrix.\n",
        "        plt.show()\n",
        "    def unison_shuffled_copies(a, b):\n",
        "        assert len(a) == len(b)\n",
        "        p = np.random.permutation(len(a))\n",
        "        return a[p], b[p]\n",
        "\n",
        "    info = INFO[data_flag_class1]\n",
        "    task = info['task']\n",
        "    _ = getattr(medmnist, INFO[data_flag_class1]['python_class'])(\n",
        "            split=\"train\", root=input_root, download=True)\n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag_class1, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "    if not os.path.isdir(output_root):\n",
        "        os.makedirs(output_root)\n",
        "\n",
        "    info = INFO[data_flag_class2]\n",
        "    task = info['task']\n",
        "    _ = getattr(medmnist, INFO[data_flag_class2]['python_class'])(\n",
        "            split=\"train\", root=input_root, download=True)\n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag_class2, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "\n",
        "    info = INFO[data_flag_class3]\n",
        "    task = info['task']\n",
        "    _ = getattr(medmnist, INFO[data_flag_class3]['python_class'])(\n",
        "            split=\"train\", root=input_root, download=True)\n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag_class3, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "    if not os.path.isdir(output_root):\n",
        "        os.makedirs(output_root)\n",
        "\n",
        "    npz_file_class1 = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag_class1)))\n",
        "    npz_file_class2 = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag_class2)))\n",
        "    npz_file_class3 = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag_class3)))\n",
        "    ood_test = npz_file_class3['train_images'][:580,...]\n",
        "    ood_test=ood_test[:,:,:,0:1]\n",
        "    print(np.shape(ood_test))\n",
        "    X_train = np.concatenate((npz_file_class1['train_images'][:580,...], npz_file_class2['train_images']), axis=0)\n",
        "    Y_train =np.concatenate((np.zeros(np.shape(npz_file_class1['train_images'][:580,...])[0]), np.ones(np.shape(npz_file_class2['train_images'])[0])), axis=0)\n",
        "    X_train,Y_train= unison_shuffled_copies(X_train, Y_train)\n",
        "    X_test = np.concatenate((npz_file_class1['test_images'][:80,...], npz_file_class2['test_images']), axis=0)\n",
        "    Y_test =np.concatenate((np.zeros(np.shape(npz_file_class1['test_images'][:80,...])[0]), np.ones(np.shape(npz_file_class2['test_images'])[0])), axis=0)\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "vRCGveSmWhtv"
      }
    }
  ]
}