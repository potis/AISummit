{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/potis/AISummit/blob/main/AI_Summit_2023_Part_1_MNIST_Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JL1f1sbm4MLb"
      },
      "source": [
        "# Part 1: Uncertainty and Out-of-Distribution detection\n",
        "\n",
        "## Motivation \n",
        "\n",
        "Machine Learning models based on Deep Neural Networks behave unpredictably when\n",
        "presented with inputs that do not stem from the training distribution and\n",
        "sometimes make egregiously wrong predictions with high confidence. Mitigating\n",
        "these errors is critical for healthcare applications, where reliability and\n",
        "trustworthiness are of utmost importance. Uncertainty estimation and Out-of-Distribution (OOD) detection\n",
        "mechanisms can be used to prevent errors by detecting inputs that are so\n",
        "dissimilar from the training set that the model can not be expected to make\n",
        "reliable predictions.\n",
        "\n",
        "This notebook will sketch out the implementation of these approaches for a\n",
        "classification problem on the MNIST dataset. **Keep in mind this is a toy\n",
        "example.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "obJxClyn4QEQ"
      },
      "source": [
        "## Install necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKxMUrCO4JaZ",
        "outputId": "7eb97bbe-a9ea-49f8-9bd3-f4d468e141ad"
      },
      "outputs": [],
      "source": [
        "!pip install pip --upgrade\n",
        "!pip install medmnist\n",
        "!pip install autokeras\n",
        "!pip install numpy\n",
        "!pip install scikeras\n",
        "!pip install -U -q --use-deprecated=legacy-resolver tf-models-official tensorflow\n",
        "!pip install tensorflow_probability\n",
        "!pip install --upgrade tf_agents"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6AUjrjBa5eVk"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mgEDLGZ4WZN"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import time\n",
        "import medmnist\n",
        "import random\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import *\n",
        "from sklearn import metrics as sklearn_metrics\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    result = y_pred\n",
        "    cf_matrix = sklearn_metrics.confusion_matrix(y_true, result)\n",
        "    cfm_labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "    cfm_values = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "    cfm_percent = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cfm_labels, cfm_values, cfm_percent)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
        "    ax.set_title('Classifier\\n\\n')\n",
        "    ax.set_xlabel('\\nPredicted Values')\n",
        "    ax.set_ylabel('Actual Values')\n",
        "    # Ticket labels - List must be in alphabetical order\n",
        "    ax.xaxis.set_ticklabels(['False', 'True'])\n",
        "    ax.yaxis.set_ticklabels(['False', 'True'])\n",
        "    # Display the visualization of the Confusion Matrix.\n",
        "    plt.show()\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) =keras.datasets.mnist.load_data()\n",
        "\n",
        "train_filter = np.where((train_labels == 0 ) | (train_labels == 5))\n",
        "test_filter = np.where((test_labels == 0) | (test_labels == 5))\n",
        "ood_filter= np.where((test_labels == 1) )\n",
        "X_train, Y_train = train_data[train_filter], train_labels[train_filter]\n",
        "Y_train[Y_train==5]=1\n",
        "Y_train_c=to_categorical(Y_train)\n",
        "X_test, Y_test = test_data[test_filter], test_labels[test_filter]\n",
        "Y_test[Y_test==5]=1\n",
        "ood_test= test_data[ood_filter]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KBLGKZiv54ZP"
      },
      "source": [
        "### Show some image examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hrIS_2MK536Z",
        "outputId": "7074f7ad-2eca-4861-8e32-ce3823ced99a"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(f'Example image {str(Y_train[2])}')\n",
        "plt.imshow(X_train[2, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.title(f'Example image {str(Y_train[-3])}')\n",
        "plt.imshow(X_train[-3, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.title(f'Out of distribution')\n",
        "\n",
        "plt.imshow(ood_test[-1, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vfkVVkf95igu"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "Let's build a simple classification model to illustrate some uncertainty quantification techniques.\n",
        "\n",
        "> Keep in mind that this is an example for demonstration purposes. The training does not follow best practices for training or performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPMAq-yh4y1o",
        "outputId": "64873419-99c4-4966-89d5-97e4876fe96e"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 28\n",
        "img_width = 28\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "class MyModel(models.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Rescaling(1./255, input_shape=(28, 28, 1)))\n",
        "        model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "        model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "        model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(16, activation='relu'))\n",
        "        model.add(layers.Dropout(.25))\n",
        "        model.add(layers.Dense(2, activation='softmax'))\n",
        "        return model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "model = MyModel()\n",
        "model.build(((None,None,None, 1)))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "              learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "epochs=5\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lVsSncOKKU0y"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "jFHZtkBTKUMW",
        "outputId": "5784ecb4-aa85-493c-b59c-f9cb89c81efc"
      },
      "outputs": [],
      "source": [
        "predictions=model(X_test)\n",
        "predictions=predictions.numpy()\n",
        "predictions_bin=predictions.argmax(axis=-1)\n",
        "plot_confusion_matrix(Y_test, predictions_bin)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6hfjbyGQEB4W"
      },
      "source": [
        "### Test model in a sample originating from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "viMnCH6eENqf",
        "outputId": "bd3d2dab-4c48-49b1-8f88-2a92ee9fb7a7"
      },
      "outputs": [],
      "source": [
        "# Get a sample\n",
        "example_case=10\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "predictions_sample=model(sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "print(np.shape(predictions_sample))\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "print('In-distribution sample')\n",
        "sample=X_test[example_case, :,:]\n",
        "plt.figure()\n",
        "plt.imshow(sample, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "print(predictions_sample)\n",
        "# What if we just rotate?\n",
        "print('Rotating image 90 degrees')\n",
        "\n",
        "sample=X_test[example_case, :,:]\n",
        "plt.figure()\n",
        "plt.imshow(np.rot90(sample), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "rot_sample=np.rot90(X_test[example_case, :,:],1).reshape((1,28,28))\n",
        "predictions_sample=model(rot_sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "print(predictions_sample)\n",
        "print('Fliping image upside down')\n",
        "plt.figure()\n",
        "plt.imshow(np.flipud(sample), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "rot_sample=np.flipud(X_test[example_case, :,:]).reshape((1,28,28))\n",
        "predictions_sample=model(rot_sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "print(predictions_sample)\n",
        "\n",
        "\n",
        "print('Invert Color')\n",
        "plt.figure()\n",
        "plt.imshow(np.abs(255-sample), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "rot_sample=np.abs(255-sample).reshape((1,28,28))\n",
        "predictions_sample=model(rot_sample)\n",
        "predictions_sample=predictions_sample.numpy()\n",
        "predictions_sample=predictions_sample[0,1]\n",
        "print(predictions_sample)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "abofgUMOlrTG"
      },
      "source": [
        "> Image augmentation could address the above issue.\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yozys_UcD7Jo"
      },
      "source": [
        "### Test model in out of distribution data (Deterministic Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "sKuiTDzWD_1o",
        "outputId": "5bdd0f52-608e-40e7-8c64-80ca699f199c"
      },
      "outputs": [],
      "source": [
        "# resnet_uncertainty = resnet_probs * (1 - resnet_probs)\n",
        "# Get a sample\n",
        "sample=ood_test[4, :,:]\n",
        "plt.figure()\n",
        "plt.imshow((sample), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "predictions=model(sample)\n",
        "predictions=predictions.numpy()\n",
        "print(f'Predicted class: {predictions.argmax()}')\n",
        "predictions=predictions[0,1]\n",
        "print(predictions)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wyYBlW5X-2kr"
      },
      "source": [
        "## Method 1: Monte Carlo Dropout\n",
        "\n",
        "- **During Training:** Regularization reduce overfitting and improve generalization error\n",
        "- **During Inference:** MC Dropout is a mainstream “free lunch” method in medical imaging for approximate Bayesian computations [ref](https://arxiv.org/pdf/2110.04286.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1VpiVoW70dQ"
      },
      "outputs": [],
      "source": [
        "num_ensemble = 10\n",
        "# Easy to implement, just change the network to training mode!\n",
        "def mc_dropout_sampling(test_examples):\n",
        "  # Enable dropout during inference.\n",
        "  return model(test_examples, training=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9zO6oNdWAUi1"
      },
      "source": [
        "### In distribution example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "X_SJuDzxAmQR",
        "outputId": "d142c7b3-54a3-4a51-a093-ba270e32e046"
      },
      "outputs": [],
      "source": [
        "# Get a sample\n",
        "sample=X_test[0,:,:]\n",
        "print(f\"True Class {Y_test[0]}\")\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "dropout_samples = [mc_dropout_sampling(sample).numpy()[0,1] for _ in range(num_ensemble)]\n",
        "print(np.array(dropout_samples).mean())\n",
        "uncertainty = np.array(dropout_samples).std()\n",
        "print(uncertainty)\n",
        "# Get a sample\n",
        "sample=X_test[10,:,:]\n",
        "print(f\"True Class {Y_test[10]}\")\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "dropout_samples = [mc_dropout_sampling(sample).numpy()[0,1] for _ in range(num_ensemble)]\n",
        "print(np.array(dropout_samples).mean())\n",
        "uncertainty = np.array(dropout_samples).std()\n",
        "print(uncertainty)\n",
        "\n",
        "plt.title(f'MC Dropout output for True Class {Y_test[10]}\"')\n",
        "plt.plot(dropout_samples)\n",
        "plt.xlabel('Run')\n",
        "plt.ylabel('Output')\n",
        "plt.ylim((0, 1.1))   # set the ylim to bottom, top\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6lfIamHSAbNJ"
      },
      "source": [
        "### Out of distribution example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "Gj0PhqQuAgBw",
        "outputId": "dea0dcb2-a301-4a25-fe7f-8165846fadac"
      },
      "outputs": [],
      "source": [
        "example=10\n",
        "plt.figure()\n",
        "plt.imshow(ood_test[example, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "sample_ood=ood_test[example,:,:]\n",
        "sample_ood=np.reshape(sample_ood,(1,28,28))\n",
        "dropout_samples_ood = [mc_dropout_sampling(sample_ood).numpy()[0,1] for _ in range(num_ensemble)]\n",
        "plt.title('MC Dropout output for an OOD example')\n",
        "plt.plot(dropout_samples_ood)\n",
        "plt.xlabel('Run')\n",
        "plt.ylabel('Output')\n",
        "plt.ylim((0, 1.1))   # set the ylim to bottom, top\n",
        "\n",
        "# print(dropout_samples_ood)\n",
        "print(\"Prediction\")\n",
        "print(np.array(dropout_samples_ood).mean())\n",
        "print(\"Uncertainty\")\n",
        "print(np.array(dropout_samples_ood).std())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1YIx03Uoj6H"
      },
      "source": [
        "> *TASK*: Try additional examples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1-www8SdCaeP"
      },
      "source": [
        "## Method 2: Deep Ensemble"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f36CWH9Rpt2v"
      },
      "source": [
        "Deep Ensembles are ensembles of DNNs trained on\n",
        "the same dataset with different random parameter initializations and data shuffling,  The ensemble\n",
        "members can be interpreted as samples from different\n",
        "modes of the Bayesian parameter posterior,\n",
        "which allows their predictions to be more diverse compared\n",
        "to unimodal approximate Bayesian approaches. They\n",
        "have been shown to be a reliable and scalable approach for\n",
        "improving both the predictive performance and quality of\n",
        "uncertainty estimates of DNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNcw40pcBg7x",
        "outputId": "949cd277-7384-4dab-9838-10ce4cc38443"
      },
      "outputs": [],
      "source": [
        "num_ensemble=10\n",
        "# Deep ensemble training\n",
        "deep_ensemble = []\n",
        "import random\n",
        "\n",
        "\n",
        "for ivar in range(num_ensemble):\n",
        "  print(f'{ivar}')\n",
        "  tf.keras.utils.set_random_seed(random.randint(100,1000))\n",
        "  temp_model= MyModel()\n",
        "  temp_model.build(((None,None,None, 1)))\n",
        "  temp_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "  temp_model.reset_states()\n",
        "  temp_model.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=epochs)\n",
        "  print(f'------')\n",
        "\n",
        "  deep_ensemble.append(temp_model)\n",
        "\n",
        "  del temp_model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oilvwfejp9BW"
      },
      "source": [
        "### In distribution example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "W0VXkXFYp8wZ",
        "outputId": "5c9d08b9-5f03-4cdc-9fac-437374b45a33"
      },
      "outputs": [],
      "source": [
        "# Get a sample\n",
        "sample=X_test[0,:,:]\n",
        "print(f\"True Class {Y_test[0]}\")\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "ensemble_preds_samples = [model_x(sample).numpy()[0,1] for model_x in (deep_ensemble)]\n",
        "print(np.array(ensemble_preds_samples).mean())\n",
        "uncertainty = np.array(ensemble_preds_samples).std()\n",
        "print(uncertainty)\n",
        "# Get a sample\n",
        "sample=X_test[10,:,:]\n",
        "print(f\"True Class {Y_test[10]}\")\n",
        "sample=np.reshape(sample,(1,28,28))\n",
        "ensemble_preds_samples = [model_x(sample).numpy()[0,1] for model_x in (deep_ensemble)]\n",
        "print(np.array(ensemble_preds_samples).mean())\n",
        "uncertainty = np.array(ensemble_preds_samples).std()\n",
        "print(uncertainty)\n",
        "plt.title(f'Deep Ensembles output for True Class {Y_test[10]}\"')\n",
        "plt.plot(ensemble_preds_samples)\n",
        "plt.xlabel('Run')\n",
        "plt.ylabel('Output')\n",
        "plt.ylim((0, 1.1))   # set the ylim to bottom, top\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TNvDFCLfp1ww"
      },
      "source": [
        "### Out of distribution example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "pi9HqeKBp2aD",
        "outputId": "1b4f66fe-f4c4-451a-deca-27a3a558c9d8"
      },
      "outputs": [],
      "source": [
        "example=11\n",
        "plt.figure()\n",
        "plt.imshow(ood_test[example, :,:], cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "sample_ood=ood_test[example,:,:]\n",
        "sample_ood=np.reshape(sample_ood,(1,28,28))\n",
        "ensemble_preds_samples = [model_x(sample).numpy()[0,1] for model_x in (deep_ensemble)]\n",
        "plt.title('Deep Ensemble output for an OOD example')\n",
        "plt.plot(ensemble_preds_samples)\n",
        "plt.xlabel('Run')\n",
        "plt.ylabel('Output')\n",
        "plt.ylim((0, 1.1))   # set the ylim to bottom, top\n",
        "\n",
        "# print(dropout_samples_ood)\n",
        "print(\"Prediction\")\n",
        "print(np.array(ensemble_preds_samples).mean())\n",
        "print(\"Uncertainty\")\n",
        "print(np.array(ensemble_preds_samples).std())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JybeY-om26XT"
      },
      "source": [
        "## Method 3: SNGP Spectral-normalized Neural Gaussian Process\n",
        "\n",
        "SNGP is a simple approach to improve a deep classifier's uncertainty quality while maintaining a similar level of accuracy and latency. Given a deep residual network, SNGP makes two simple changes to the model:\n",
        "\n",
        "- It applies spectral normalization to the hidden residual layers.\n",
        "- It replaces the Dense output layer with a Gaussian process layer.\n",
        "\n",
        "\n",
        "#### Advantages\n",
        "It is a single-model method—it does not rely on ensemble averaging. Therefore, SNGP has a similar level of latency as a single deterministic network, and can be scaled easily to large datasets\n",
        "It has strong out-of-domain detection performance due to the distance-awareness property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXGdSvnk_w0e",
        "outputId": "eee490a7-5046-4760-9fc2-d187af6f32fb"
      },
      "outputs": [],
      "source": [
        "import official.nlp.modeling.layers as nlp_layers\n",
        "epochs=10\n",
        "class simplenet(tf.keras.Model):\n",
        "    \"\"\"Defines a multi-layer residual network.\"\"\"\n",
        "    def __init__(self, num_classes=1, num_layers=1, num_hidden=32,\n",
        "                 dropout_rate=0.1, **classifier_kwargs):\n",
        "        super().__init__()\n",
        "        # Define class meta data.\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.classifier_kwargs = classifier_kwargs\n",
        "\n",
        "        # Define the layers.\n",
        "        self.input_layer = tf.keras.layers.Input((None,None))\n",
        "        self.normalize_layer = tf.keras.layers.Rescaling(1/255.)\n",
        "        self.reshape_layer = tf.keras.layers.Reshape((28, 28, 1))\n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')\n",
        "        self.maxpool1 = tf.keras.layers.MaxPool2D(2)\n",
        "        self.conv2 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "        self.maxpool2 = tf.keras.layers.MaxPool2D(2)\n",
        "        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')\n",
        "        self.maxpool3 = tf.keras.layers.MaxPool2D(2)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(num_hidden)\n",
        "        self.dense_layers = [self.make_dense_layer() for _ in range(num_layers)]\n",
        "        self.classifier = self.make_output_layer(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Project the 2D input data to high dimension.\n",
        "        normalized = self.normalize_layer(inputs)\n",
        "        reshape_layer=self.reshape_layer(normalized)\n",
        "        conv1 = self.conv1(reshape_layer)\n",
        "        maxpool1 = self.maxpool1(conv1)\n",
        "        conv2 = self.conv2(maxpool1)\n",
        "        maxpool2 = self.maxpool2(conv2)\n",
        "        conv3 = self.conv3(maxpool2)\n",
        "        maxpool3 = self.maxpool3(conv3)\n",
        "        flattened = self.flatten(maxpool3)\n",
        "        dense = self.dense(flattened)\n",
        "\n",
        "        # Compute the ResNet hidden representations.\n",
        "        for i in range(self.num_layers):\n",
        "            resid = self.dense_layers[i](dense)\n",
        "            resid = tf.keras.layers.Dropout(self.dropout_rate)(resid)\n",
        "            dense += resid\n",
        "\n",
        "        return self.classifier(dense)\n",
        "\n",
        "    def make_dense_layer(self):\n",
        "        \"\"\"Use the Dense layer as the hidden layer.\"\"\"\n",
        "        return tf.keras.layers.Dense(self.num_hidden, activation=\"relu\")\n",
        "\n",
        "    def make_output_layer(self, num_classes):\n",
        "        \"\"\"Use the Dense layer as the output layer.\"\"\"\n",
        "        return tf.keras.layers.Dense(\n",
        "            num_classes, **self.classifier_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2HtZTOlKqm1",
        "outputId": "61c6b366-2c93-423e-d581-f09683fab029"
      },
      "outputs": [],
      "source": [
        "class simplenetSNGP(simplenet):\n",
        "  def __init__(self, spec_norm_bound=0.9, **kwargs):\n",
        "    self.spec_norm_bound = spec_norm_bound\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def make_dense_layer(self):\n",
        "    \"\"\"Applies spectral normalization to the hidden layer.\"\"\"\n",
        "    dense_layer = super().make_dense_layer()\n",
        "    return nlp_layers.SpectralNormalization(\n",
        "        dense_layer, norm_multiplier=self.spec_norm_bound)\n",
        "\n",
        "  def make_output_layer(self, num_classes):\n",
        "    \"\"\"Uses Gaussian process as the output layer.\"\"\"\n",
        "    return nlp_layers.RandomFeatureGaussianProcess(\n",
        "        2,\n",
        "        gp_cov_momentum=-1,\n",
        "        **self.classifier_kwargs)\n",
        "\n",
        "  def call(self, inputs, training=False, return_covmat=False):\n",
        "    # Gets logits and a covariance matrix from the GP layer.\n",
        "    logits, covmat = super().call(inputs)\n",
        "\n",
        "    # Returns only logits during training.\n",
        "    if not training and return_covmat:\n",
        "      return logits, covmat\n",
        "\n",
        "    return logits\n",
        "# sngp_model = simplenetSNGP(**resnet_config)\n",
        "# sngp_model.build((None, None))\n",
        "# sngp_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResetCovarianceCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    \"\"\"Resets covariance matrix at the beginning of the epoch.\"\"\"\n",
        "    if epoch > 0:\n",
        "      self.model.classifier.reset_covariance_matrix()\n",
        "class DeepResNetSNGPWithCovReset(simplenetSNGP):\n",
        "  def fit(self, *args, **kwargs):\n",
        "    \"\"\"Adds ResetCovarianceCallback to model callbacks.\"\"\"\n",
        "    kwargs[\"callbacks\"] = list(kwargs.get(\"callbacks\", []))\n",
        "    kwargs[\"callbacks\"].append(ResetCovarianceCallback())\n",
        "\n",
        "    return super().fit(*args, **kwargs)\n",
        "resnet_config = dict(num_classes=2)\n",
        "sngp_model = simplenetSNGP(**resnet_config)\n",
        "sngp_model.build((None,None, None,1))\n",
        "sngp_model.summary()\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "metrics = tf.keras.metrics.CategoricalAccuracy(),\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
        "train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "sngp_model.compile(**train_config)\n",
        "sngp_model.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaPQml4OLN0X"
      },
      "outputs": [],
      "source": [
        "def compute_posterior_mean_probability(logits, covmat, lambda_param=np.pi / 8.):\n",
        "  # Computes uncertainty-adjusted logits using the built-in method.\n",
        "  logits_adjusted = nlp_layers.gaussian_process.mean_field_logits(\n",
        "      logits, covmat, mean_field_factor=lambda_param)\n",
        "\n",
        "  return tf.nn.softmax(logits_adjusted, axis=-1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In distribution example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2QM6Adk9s7X8",
        "outputId": "2e7bf986-e4db-4e5b-8476-aa39dc22501c"
      },
      "outputs": [],
      "source": [
        "test_filter = np.where((Y_test == 1))\n",
        "X_test_0=X_test[test_filter]\n",
        "sngp_logits, sngp_covmat = sngp_model(X_test_0, return_covmat=True)\n",
        "sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)\n",
        "sngp_probs=sngp_probs[:,1]\n",
        "# print(np.shape(sngp_probs))\n",
        "# print((sngp_probs))\n",
        "# uncertainty = sngp_probs * (1. - sngp_probs)\n",
        "# print((uncertainty))\n",
        "plt.plot(sngp_probs)\n",
        "uncertainty = sngp_probs * (1. - sngp_probs)\n",
        "plt.plot(uncertainty, c='r')\n",
        "plt.title('Test Set')\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X0y5QErq2sFF"
      },
      "source": [
        "### Out of distribution example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY_i79Ni2rBj"
      },
      "outputs": [],
      "source": [
        "sngp_logits_ood, sngp_covmat_ood = sngp_model(ood_test, return_covmat=True)\n",
        "sngp_probs_ood = compute_posterior_mean_probability(sngp_logits_ood, sngp_covmat_ood)[:,1]\n",
        "print(np.shape(sngp_probs_ood))\n",
        "# print((sngp_probs_ood))\n",
        "# uncertainty = sngp_probs_ood * (1. - sngp_probs_ood)\n",
        "# print((uncertainty))\n",
        "\n",
        "plt.imshow(ood_test[80,:,:])\n",
        "plt.show()\n",
        "plt.plot(sngp_probs_ood)\n",
        "uncertainty = sngp_probs_ood * (1. - sngp_probs_ood)\n",
        "plt.plot(uncertainty, c='r')\n",
        "plt.title('OOD Set')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "8rQ1trmvZAZZ",
        "outputId": "ac5a4cd7-cbaf-4a44-b3e9-d6e401721ce4"
      },
      "outputs": [],
      "source": [
        "sngp_logits, sngp_covmat = sngp_model(X_test, return_covmat=True)\n",
        "sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(Y_test,sngp_probs.numpy().argmax(axis=-1) )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-B-md4PW2JdK"
      },
      "source": [
        "## Method 4: Bayesian Neural Network -Variational inference (VI) as an approximative Bayes approach"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1wwM3YUW2WGy"
      },
      "source": [
        "Now we will train a bayesian neural network via variational inference. We again use a CNN with two convolutional blocks, followed by maxpooling layers. The setting is the same as above.\n",
        "\n",
        "\n",
        "\n",
        "The main idea of the Bayes approach in DL is that with BNNs, each weight is replaced\n",
        "by a distribution. Normally, this is quite a complicated distribution, and this distribution\n",
        "isn’t independent among different weights. The idea behind the VI Bayes\n",
        "method is that the complicated posterior distributions of the weights are approximated\n",
        "by a simple distribution called variational distribution."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bBQWqbO-2MOO"
      },
      "source": [
        "### In distribution example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HeMls-rS8sC",
        "outputId": "4be954c9-faf2-4a4c-b4e7-6b7c62af04cf"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "disable_eager_execution()\n",
        "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X_train.shape[0] *1.0)\n",
        "\n",
        "model_vi = Sequential()\n",
        "model_vi.add(layers.Rescaling(1./255, input_shape=(28, 28, 1)))\n",
        "model_vi.add(tfp.layers.Convolution2DFlipout(16,kernel_size=(3,3),padding=\"same\", activation = 'relu', kernel_divergence_fn=kernel_divergence_fn,input_shape=(28,28,1)))\n",
        "model_vi.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "model_vi.add(tfp.layers.Convolution2DFlipout(32,kernel_size=(3,3),padding=\"same\", activation = 'relu', kernel_divergence_fn=kernel_divergence_fn))\n",
        "model_vi.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "model_vi.add(tfp.layers.Convolution2DFlipout(64,kernel_size=(3,3),padding=\"same\", activation = 'relu', kernel_divergence_fn=kernel_divergence_fn))\n",
        "model_vi.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "model_vi.add(tf.keras.layers.Flatten())\n",
        "model_vi.add(tfp.layers.DenseFlipout(32, activation = 'relu', kernel_divergence_fn=kernel_divergence_fn))\n",
        "model_vi.add(tfp.layers.DenseFlipout(2, activation = 'softmax', kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "X_train=X_train.reshape((-1,28,28,1))\n",
        "\n",
        "model_vi.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['categorical_accuracy'])\n",
        "epochs=10\n",
        "print(np.shape(Y_train))\n",
        "\n",
        "print(np.shape(Y_train))\n",
        "model_vi.fit(\n",
        "    X_train,\n",
        "    Y_train_c,\n",
        "    validation_split=0.3,\n",
        "    epochs=epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "QaLrg6rq7f8s",
        "outputId": "1a24c4b7-4451-4ade-ab51-a0c5e8821176"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(0,50):\n",
        "  plt.scatter(range(0,2),model_vi.predict(X_test[0:1].reshape(1,28,28,1)),c=\"blue\",alpha=0.2)\n",
        "plt.xticks(range(0,2),labels=np.repeat(\" \",2))\n",
        "plt.ylim([0,1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QGkmIqHwxpeH"
      },
      "source": [
        "### Out of distribution example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "F7WRyJqsxtDi",
        "outputId": "531cba18-5e26-48f9-c4ee-b140bbfa5b4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(0,50):\n",
        "  plt.scatter(range(0,2),model_vi.predict(ood_test[0:1].reshape(1,28,28,1)),c=\"blue\",alpha=0.2)\n",
        "plt.xticks(range(0,2),labels=np.repeat(\" \",2))\n",
        "plt.ylim([0,1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vRCGveSmWhtv"
      },
      "source": [
        "# Whats Next?\n",
        "\n",
        "\n",
        "1. Try the same experiment with different digits!\n",
        "2. What about a medical dataset?\n",
        "  - Create a copy of the notebook\n",
        "  - Use the following code to load the data\n",
        "  ```\n",
        "    import os\n",
        "    import matplotlib.pyplot as plt\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    import numpy as np\n",
        "    import time\n",
        "    import medmnist\n",
        "    import random\n",
        "    from sklearn.calibration import CalibratedClassifierCV\n",
        "    from medmnist import INFO, Evaluator\n",
        "    from medmnist.info import DEFAULT_ROOT\n",
        "    from sklearn.metrics import *\n",
        "    from sklearn import metrics as sklearn_metrics\n",
        "    import seaborn as sns\n",
        "    data_flag_class3 = \"pathmnist\"\n",
        "    data_flag_class2 = \"breastmnist\"\n",
        "    data_flag_class1 = \"pneumoniamnist\"\n",
        "\n",
        "    output_root =\"./ood\"\n",
        "\n",
        "    input_root = DEFAULT_ROOT\n",
        "\n",
        "    def plot_confusion_matrix(y_true, y_pred):\n",
        "        result = np.zeros(np.shape(y_pred)[0])\n",
        "        result[y_pred[:,0] < 0.5] = 0\n",
        "        result[y_pred[:,0] >= 0.5] = 1\n",
        "        cf_matrix = sklearn_metrics.confusion_matrix(y_true, result)\n",
        "\n",
        "        cfm_labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
        "        cfm_values = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "        cfm_percent = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cfm_labels, cfm_values, cfm_percent)]\n",
        "        labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "        ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
        "        ax.set_title('Classifier\\n\\n')\n",
        "        ax.set_xlabel('\\nPredicted Values')\n",
        "        ax.set_ylabel('Actual Values')\n",
        "\n",
        "        # Ticket labels - List must be in alphabetical order\n",
        "        ax.xaxis.set_ticklabels(['False', 'True'])\n",
        "        ax.yaxis.set_ticklabels(['False', 'True'])\n",
        "\n",
        "        # Display the visualization of the Confusion Matrix.\n",
        "        plt.show()\n",
        "    def unison_shuffled_copies(a, b):\n",
        "        assert len(a) == len(b)\n",
        "        p = np.random.permutation(len(a))\n",
        "        return a[p], b[p]\n",
        "\n",
        "    info = INFO[data_flag_class1]\n",
        "    task = info['task']\n",
        "    _ = getattr(medmnist, INFO[data_flag_class1]['python_class'])(\n",
        "            split=\"train\", root=input_root, download=True)\n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag_class1, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "    if not os.path.isdir(output_root):\n",
        "        os.makedirs(output_root)\n",
        "\n",
        "    info = INFO[data_flag_class2]\n",
        "    task = info['task']\n",
        "    _ = getattr(medmnist, INFO[data_flag_class2]['python_class'])(\n",
        "            split=\"train\", root=input_root, download=True)\n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag_class2, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "\n",
        "    info = INFO[data_flag_class3]\n",
        "    task = info['task']\n",
        "    _ = getattr(medmnist, INFO[data_flag_class3]['python_class'])(\n",
        "            split=\"train\", root=input_root, download=True)\n",
        "\n",
        "    output_root = os.path.join(output_root, data_flag_class3, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "    if not os.path.isdir(output_root):\n",
        "        os.makedirs(output_root)\n",
        "\n",
        "    npz_file_class1 = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag_class1)))\n",
        "    npz_file_class2 = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag_class2)))\n",
        "    npz_file_class3 = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag_class3)))\n",
        "    ood_test = npz_file_class3['train_images'][:580,...]\n",
        "    ood_test=ood_test[:,:,:,0:1]\n",
        "    print(np.shape(ood_test))\n",
        "    X_train = np.concatenate((npz_file_class1['train_images'][:580,...], npz_file_class2['train_images']), axis=0)\n",
        "    Y_train =np.concatenate((np.zeros(np.shape(npz_file_class1['train_images'][:580,...])[0]), np.ones(np.shape(npz_file_class2['train_images'])[0])), axis=0)\n",
        "    X_train,Y_train= unison_shuffled_copies(X_train, Y_train)\n",
        "    X_test = np.concatenate((npz_file_class1['test_images'][:80,...], npz_file_class2['test_images']), axis=0)\n",
        "    Y_test =np.concatenate((np.zeros(np.shape(npz_file_class1['test_images'][:80,...])[0]), np.ones(np.shape(npz_file_class2['test_images'])[0])), axis=0)\n",
        "  ```\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMQ2GStcIK8yXTlbjzOQzlA",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
